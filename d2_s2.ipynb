{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dressed-makeup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td class=\"sessionId\">2</td><td>None</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/spark/erin/40409/jobs\">Link</a></td><td></td><td>erin</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=spark.sql(\"select * from user_erin.review\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-immune",
   "metadata": {},
   "source": [
    "## 机器学习：文本特征提取\n",
    "- N-Gram认为语言中每个单词只与其前面长度 N-1 的上下文有关。主要分为bigram和trigram，bigram假设下一个词的出现依赖它前面的一个词，trigram假设下一个词的出现依赖它前面的两个词。在SparkML中用NGram类实现，setN(2)为bigram，setN(3)为trigram。\n",
    "- pyspark不能自动结合不同n-gram的特征，所以需要用VectorAssembler来结合。在build_ngrams_wocs，它需要的输入列为您准备训练的文字项列名与结果项列名。输入经过转化后会生成输出项featuresn。\n",
    "- 以下的vocabsize定为5000，featuresn会从unigram，bigram，trigram中分别获得5000个特征，经过合并后，featuresn会得到15000个特征。您可以通过改变vocabsize来改变您需要得到的文本特征数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "right-agriculture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocabsize=5000\n",
    "from pyspark.ml.feature import NGram, VectorAssembler\n",
    "def build_ngrams_wocs(inputCol=[\"only_str\",\"score\"], n=3):\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"filtered\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=vocabsize,inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"featuresn\",\n",
    "    )]\n",
    "    return Pipeline(stages=ngrams + cv + idf+ assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "silent-miracle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer,CountVectorizer,IDF\n",
    "from pyspark.ml import Pipeline\n",
    "trigramwocs_pipelineFit = build_ngrams_wocs().fit(df)\n",
    "df= trigramwocs_pipelineFit.transform(df)\n",
    "df = df.na.fill(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "congressional-stranger",
   "metadata": {},
   "source": [
    "## 机器学习：将文本类特征项和数字类特征项结合\n",
    "- 运用VectorAssembler将您需要结合的列名填入，例如下面inputCols的队列里填入了5个数字类列名和文本类的特征项。生成的新列名在outputCol中，为“newnfeatures”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "noticed-gateway",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "va = VectorAssembler(inputCols=[\"user\",\"product\",\"helpfulnessnumerator\",\"helpfulnessdenominator\",\"time\",\"featuresn\"], outputCol=\"newnfeatures\")\n",
    "df= va.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dominant-clock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=df.select(\"id\",\"productid\",\"userid\",\"newnfeatures\",\"score\")\n",
    "df.write.saveAsTable(\"user_erin.food_features\", format=\"orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "employed-period",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
