{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elementary-photograph",
   "metadata": {},
   "source": [
    "## 数据采集：\n",
    "- 以下这个单元格的功能是将您上传的csv文件读入到pyspark程序里，并将其的格式转换为pyspark dataframe。\n",
    "- 您可以通过改变spark.read.csv()中的路径来读入您本人上传的数据集。\n",
    "- data.show()是将dataframe的内容展示出来的方程。展示出来的数据集默认是前20行，您可以通过在show（）中填写数字，来改变展示的行数。例如show（5），会展示数据集中的前5行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greenhouse-skating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "| id| productid|        userid|         profilename|helpfulnessnumerator|helpfulnessdenominator|score|      time|             summary|                text|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "|  1|B001E4KFG0|A3SGXH7AUHU8GW|          delmartian|                   1|                     1|    5|1303862400|Good Quality Dog ...|I have bought sev...|\n",
      "|  2|B00813GRG4|A1D87F6ZCVE5NK|              dll pa|                   0|                     0|    1|1346976000|   Not as Advertised|\"Product arrived ...|\n",
      "|  5|B006K2ZZ7K|A1UQRSCLF8GW1T|\"Michael D. Bigha...|                   0|                     0|    5|1350777600|         Great taffy|Great taffy at a ...|\n",
      "|  8|B006K2ZZ7K|A3JRGQVEQN31IQ|  Pamela G. Williams|                   0|                     0|    5|1336003200|Wonderful, tasty ...|This taffy is so ...|\n",
      "|  9|B000E7L2R4|A1MZYO9TZK0BBI|            R. James|                   1|                     1|    5|1322006400|          Yay Barley|Right now I'm mos...|\n",
      "| 11|B0001PB9FE|A3HDKO7OW0QNK4|        Canadian Fan|                   1|                     1|    5|1107820800|The Best Hot Sauc...|I don't know if i...|\n",
      "| 16|B001GVISJM|A1CZX3CP8IKQIJ|        Brian A. Lee|                   4|                     5|    5|1262044800|Lots of twizzlers...|My daughter loves...|\n",
      "| 20|B001GVISJM|A3IV7CL2C13K2U|                Greg|                   0|                     0|    5|1318032000|Home delivered tw...|Candy was deliver...|\n",
      "| 25|B001GVISJM|A22P2J09NJ9HKE|\"S. Cabanaugh \"\"j...|                   0|                     0|    5|1295481600|Please sell these...|I have lived out ...|\n",
      "| 26|B001GVISJM|A3FONPR03H3PJS|\"Deborah S. Linze...|                   0|                     0|    5|1288310400|Twizzlers - Straw...|\"Product received...|\n",
      "| 27|B001GVISJM|A3RXAU2N8KV45G|              lady21|                   0|                     1|    1|1332633600|     Nasty No flavor|The candy is just...|\n",
      "| 31|B003F6UO7K| AFM0O9480F04W|             Sherril|                   0|                     0|    5|1297641600|      Great machine!|I have never been...|\n",
      "| 37|B001EO5QW8|A1MYS9LFFBIYKM|\"Abby Chase \"\"glu...|                   2|                     2|    5|1190851200|Love Gluten Free ...|For those of us w...|\n",
      "| 42|B001EO5QW8|A1WK4ALVZDYPUE|\"Dick Baldwin \"\"c...|                   0|                     0|    5|1302134400|Oatmeal For Oatme...|McCann's makes oa...|\n",
      "| 45|B001EO5QW8|A2G7B7FKP2O2PU|          D. Leschke|                   0|                     0|    5|1209686400|Great taste and c...|We really like th...|\n",
      "| 46|B001EO5QW8|A39Z97950MCTQE|         K. A. Freel|                   0|                     0|    3|1205193600|      Hearty Oatmeal|This seems a litt...|\n",
      "| 57|B004N5KULM|A202WR509428VF|amateur amazon sh...|                   2|                     2|    5|1322438400|       Awesome Deal!|Deal was awesome!...|\n",
      "| 58|B004N5KULM| ASCNNAJU6SXF8|             S. Beck|                   1|                     1|    5|1336176000|How can you go wr...|It is chocolate, ...|\n",
      "| 62|B004N5KULM| A7ZK2A3VIW7X9|               Peggy|                   0|                     2|    5|1319414400|    pretty expensive|This bag of candy...|\n",
      "| 65|B005DUM9UQ|A2XIHNXODNZGV4|\"ChemProf \"\"chem ...|                   1|                     1|    5|1333584000|great source of e...|This product serv...|\n",
      "+---+----------+--------------+--------------------+--------------------+----------------------+-----+----------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data=spark.sql(\"select * from user_erin.ratingsdemo1052\")\n",
    "data=data.filter(data.text!=\"Text\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-report",
   "metadata": {},
   "source": [
    "## 数据处理：1.处理重复数据\n",
    "- 整行去重：data.distinct()这个方程可以对于完全相同的行进行去重。\n",
    "- 对于某一列或者多列相同情况，进行去重： 您可以在dropDuplicates（）这个方程中，在队列中，填入您需要筛选的相同情况。例如以下单元格筛选了三个条件的情况，在产品，用户，评论时间都一样的情况下，删除掉重复行（[\"product\", \"user\",\"Time\"]）。\n",
    "- dataframe.count()方程会打印出这个列表的行数，以便于您了解去重之后的数据集行数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "simple-freight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170405"
     ]
    }
   ],
   "source": [
    "data=data.distinct()\n",
    "data = data.dropDuplicates(subset=[c for c in data.columns if c in [\"productid\", \"userid\",\"time\"]])\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-macro",
   "metadata": {},
   "source": [
    "## 数据处理：2.处理空缺数据\n",
    "- 运用agg方程算出每列空缺的数据比\n",
    "- 若某列的空缺比重过大，可以将该列从数据中删除。因为data的空缺比均很小，所以使用df_miss来展示如何进行某列的删除，但后续处理中并不使用df_miss。这里删除的列为“Time”，您可以将“Time”换成您需要删除的列名。\n",
    "- 删除掉结果项空缺的数据（data.na.drop(subset=['Score'])），这个数据分析主要是对于“Score”项，当Score缺失，该行对于后续分析则没有帮助。\n",
    "- 删除掉空缺项过多的行。您可以通过利用thresh参数，为每一行缺失数据的数量指定一个阈值，从而限定要删除的行。以下我们选择的阈值为3，则删除掉空缺项等于或大约3的行。您可以改变thresh对应的值来改变阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dominant-negotiation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+--------------+-------------------+----------------------------+------------------------------+-------------+------------+---------------+------------+\n",
      "|id_missing|productid_missing|userid_missing|profilename_missing|helpfulnessnumerator_missing|helpfulnessdenominator_missing|score_missing|time_missing|summary_missing|text_missing|\n",
      "+----------+-----------------+--------------+-------------------+----------------------------+------------------------------+-------------+------------+---------------+------------+\n",
      "|       0.0|              0.0|           0.0|                0.0|                         0.0|                           0.0|          0.0|         0.0|            0.0|         0.0|\n",
      "+----------+-----------------+--------------+-------------------+----------------------------+------------------------------+-------------+------------+---------------+------------+"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "data.agg(*[(1-(f.count(c) /f.count('*'))).alias(c+'_missing') for c in data.columns]).show()\n",
    "data=data.na.drop(subset=['score'])\n",
    "data=data.dropna(thresh=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-campus",
   "metadata": {},
   "source": [
    "## 数据处理：3.将某列数据的类别进行转化\n",
    "- 当您直接用spark的read_csv导入数据时，生成的dataframe往往会默认每列的类别为string，然而string类别会让您接下来的数据分析以及训练的步骤报错。\n",
    "- 以下这个单元格将四列的数据种类进行了改变。您可以通过在withColumn这个方程来进行改变，在括号中填入您想要改变的列名，以及想改变成的种类形式。形式：（“列名”,dataframe[\"列名\"].cast('type')）\n",
    "- 以下为一些您可能需要使用的类别：binary;boolean;int;string;float;timestamp;date;float。您可以根据数据种类将以上类别替换到'type'里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "annoying-monte",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = data.withColumn(\"helpfulnessnumerator\",data['helpfulnessnumerator'].cast('int'))\n",
    "data = data.withColumn(\"helpfulnessdenominator\",data['helpfulnessdenominator'].cast('int'))\n",
    "data = data.withColumn(\"score\",data['score'].cast('int'))\n",
    "data = data.withColumn(\"time\",data['time'].cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-payroll",
   "metadata": {},
   "source": [
    "## 数据处理：4.把类别型数据转化为数字型数据 \n",
    "\n",
    "- 将数据库中的种类值转化为数字，例如“south”，“north”，“east”，“west”，转换为“1”，“2”，“3”，“4”，以方便后续训练模型。\n",
    "- 以下这个单元格的主要功能是将‘productId’和‘UserId’的值由一串字符转换为数字。\n",
    "- StringIndexer这个方程的使用方法为，在inputCol后填入您需要转化的列名。然后用生成的模型（product_indexer）来transform原数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "equivalent-wallet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "product_indexer = StringIndexer(inputCol='productid', outputCol='product').fit(data)\n",
    "data= product_indexer.transform(data)\n",
    "user_indexer = StringIndexer(inputCol='userid', outputCol='user').fit(data)\n",
    "data= user_indexer.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-thunder",
   "metadata": {},
   "source": [
    "## 数据处理：5.将包含不符合打分要求的结果项的行删除\n",
    "- 首先将数据集根据Score项的不同值进行分类，并把每类的数量列出，再筛选出合理的Score值，并删掉Score值在合理范围外的行。\n",
    "- 您可以通过修改groupBy（)中的列名来选择依照分类的结果项。\n",
    "- 对于筛选结果项的范围，您可以将合理的结果值填入filter（）方程中，例如（data.Score==0），并且将这些等式用‘|’隔开，表示并列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "laughing-nelson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|score|count |\n",
      "+-----+------+\n",
      "|5    |108269|\n",
      "|4    |24044 |\n",
      "|1    |15822 |\n",
      "|3    |12748 |\n",
      "|2    |9037  |\n",
      "|0    |327   |\n",
      "|null |78    |\n",
      "|6    |14    |\n",
      "|10   |6     |\n",
      "|9    |5     |\n",
      "|8    |5     |\n",
      "|7    |5     |\n",
      "|14   |4     |\n",
      "|47   |4     |\n",
      "|15   |3     |\n",
      "|21   |3     |\n",
      "|16   |3     |\n",
      "|19   |3     |\n",
      "|69   |2     |\n",
      "|24   |2     |\n",
      "+-----+------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "data.groupBy(\"score\").count().orderBy(col(\"count\").desc()).show(truncate=False)\n",
    "data=data.filter((data.score == 0)|(data.score == 1)|(data.score == 2) | (data.score == 3) | (data.score == 4)|(data.score == 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-productivity",
   "metadata": {},
   "source": [
    "## 数据处理：6.将文字型数据中的数字删除转化为纯文字数据\n",
    "- 您可以在withColumn（）方程中填入经过处理后的新列名（“only_str”），在regexp_replace中填入您需要处理的文字项数据列名（'Summary'）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bright-helena",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "data = data.withColumn(\"only_str\",regexp_replace(col('summary'), '\\d+', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-flash",
   "metadata": {},
   "source": [
    "## 数据处理： 7.将纯文字数据用转化成主要词队列\n",
    "- 运用RegexTokenizer将文字列转化成小写单词地队列，再运用StopWordsRemover将单词队列里的停用字（介词，代词等没有意义的词汇）删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "american-atlas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|              nwords|            filtered|\n",
      "+--------------------+--------------------+\n",
      "|[love, the, book,...|[love, book, miss...|\n",
      "|     [canine, crack]|     [canine, crack]|\n",
      "|[lots, of, crispy...|[lots, crispy, sw...|\n",
      "|[i, love, this, m...|       [love, movie]|\n",
      "|[flea, s, are, st...|       [flea, stuck]|\n",
      "|   [needs, improved]|   [needs, improved]|\n",
      "|[iexcl, oye, trem...|[iexcl, oye, trem...|\n",
      "|[read, the, fine,...| [read, fine, print]|\n",
      "|[as, good, as, we...|   [good, barcelona]|\n",
      "|[doesn, t, taste,...|[doesn, taste, li...|\n",
      "|[poor, product, q...|[poor, product, q...|\n",
      "|[pleasantly, surp...|[pleasantly, surp...|\n",
      "|[a, disappointed,...|[disappointed, bo...|\n",
      "|[excellent, produ...|[excellent, produ...|\n",
      "|[really, hot, but...|[really, hot, rea...|\n",
      "|[absolutely, superb]|[absolutely, superb]|\n",
      "|[not, the, right,...|[right, bottle, s...|\n",
      "|[omaha, cheesecak...|[omaha, cheesecak...|\n",
      "|[slightly, remini...|[slightly, remini...|\n",
      "|    [mexican, mocha]|    [mexican, mocha]|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import  RegexTokenizer,StopWordsRemover\n",
    "regex_tokenizer = RegexTokenizer(inputCol=\"only_str\", outputCol=\"nwords\", pattern=\"\\\\W\")\n",
    "data = regex_tokenizer.transform(data)\n",
    "remover= StopWordsRemover(inputCol=\"nwords\", outputCol=\"filtered\")\n",
    "data = remover.transform(data)\n",
    "data.select(\"nwords\",\"filtered\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "better-entry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.write.format(\"hive\").mode(\"overwrite\").saveAsTable(\"user_erin.review\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "automated-casino",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
